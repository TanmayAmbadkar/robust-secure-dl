{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e536dd58-69f7-41bb-939d-5ac63818d59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from CNN.resnet import ResNet18\n",
    "from load_data import load_data\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# load the mnist dataset (images are resized into 32 * 32)\n",
    "training_set, test_set = load_data(data='mnist')\n",
    "\n",
    "# define the model\n",
    "model = ResNet18()\n",
    "\n",
    "# detect if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "\n",
    "# load the learned model parameters\n",
    "model.load_state_dict(torch.load('./model_weights/cpu_model.pth'))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb0ad58e-a9ab-4976-8f43-5458763cee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, label):\n",
    "    \n",
    "    log_probs = F.log_softmax(logits, dim=1).reshape(-1,)\n",
    "    label_log_prob = log_probs[label].item()\n",
    "    log_probs[label] = -1e8\n",
    "    max_log_prob = log_probs.max()\n",
    "    return max(label_log_prob - max_log_prob, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f2ffbb-a82a-4487-b6cb-02482783bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient(network, image, label, row, col):\n",
    "    \n",
    "    constant = torch.zeros(*image.shape).to(device)\n",
    "    constant[0, 0, row:row+3, col:col+3] = 0.0001\n",
    "    diff = loss_fn(network(image+constant), label) - loss_fn(network(image-constant), label)\n",
    "    diff = diff/0.0002\n",
    "    \n",
    "    del constant\n",
    "    return diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0928fd60-4e80-4587-8481-d0dcf9bf0796",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hessian(network, image, label, row, col):\n",
    "        \n",
    "    constant = torch.zeros(*image.shape).to(device)\n",
    "    constant[0,0,row:row+3, col:col+3] = 0.0001\n",
    "    diff = loss_fn(network(image+constant), label) + loss_fn(network(image-constant), label) - 2*loss_fn(network(image), label)\n",
    "    diff = diff/0.0001**2\n",
    "    del constant\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7405363a-adb5-4c95-89ff-4f4f60852a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def zoo_attack_newton(network, image, label):\n",
    "    step_size = 0.01\n",
    "    while loss_fn(network(image), label)>0:\n",
    "        row = random.randint(0, 31-9)\n",
    "        col = random.randint(0, 31-9)\n",
    "        grad = gradient(network, image, label, row, col)\n",
    "        hess = hessian(network, image, label, row, col)\n",
    "        if hess<=0:\n",
    "            delta = -step_size*grad\n",
    "        else:\n",
    "            delta = -step_size*grad/hess\n",
    "        image[0, 0, row:row+3, col:col+3] = image[0, 0, row:row+3, col:col+3] + delta.to(device)\n",
    "        \n",
    "        print('\\rloss : %.4f'%loss_fn(network(image), label), end=\"\")\n",
    "        \n",
    "        del delta\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6015ec7-e65d-4e2d-9ea6-52b3bff2abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def zoo_attack_adam(network, image, label):\n",
    "    betas = (0.9,0.999)\n",
    "    eps = 1e-08\n",
    "    step_size = 0.01\n",
    "    M,v,T = torch.zeros(*image.shape[2:]), torch.zeros(*image.shape[2:]), torch.zeros(*image.shape[2:])\n",
    "    curr_loss = 100\n",
    "    while curr_loss>0:\n",
    "    \n",
    "        row = random.randint(0, 31-9)\n",
    "        col = random.randint(0, 31-9)\n",
    "        grad = gradient(network, image, label, row, col).cpu()\n",
    "        T[row:row+3, col:col+3]+=1\n",
    "        M[row:row+3, col:col+3] = betas[0]*M[row:row+3, col:col+3] + (1-betas[0])*grad\n",
    "        v[row:row+3, col:col+3] = betas[1]*v[row:row+3, col:col+3] + (1-betas[1])*grad**2\n",
    "        M_hat = M[row:row+3, col:col+3]/(1-betas[0]**T[row:row+3, col:col+3])\n",
    "        v_hat = v[row:row+3, col:col+3]/(1-betas[1]**T[row:row+3, col:col+3])\n",
    "        delta = -step_size * M_hat/(v_hat**0.5 + eps)\n",
    "        image[0, 0, row:row+3, col:col+3] = image[0, 0, row:row+3, col:col+3] + delta\n",
    "        curr_loss = loss_fn(network(image), label)\n",
    "        \n",
    "        print('\\rloss : %.4f'%loss_fn(network(image), label), end=\"\")\n",
    "    # print(T.sum())\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6107d14-0e4d-443d-b6a7-dbfd4bb153b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#adv_image = zoo_attack(network=model, image=images, target=target_label)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m adv_image \u001b[38;5;241m=\u001b[39m \u001b[43mzoo_attack_adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m adv_image \u001b[38;5;241m=\u001b[39m adv_image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m adv_output \u001b[38;5;241m=\u001b[39m model(adv_image)\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mzoo_attack_adam\u001b[0;34m(network, image, label)\u001b[0m\n\u001b[1;32m     11\u001b[0m grad \u001b[38;5;241m=\u001b[39m gradient(network, image, label, row, col)\n\u001b[1;32m     12\u001b[0m T[row:row\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m, col:col\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 13\u001b[0m M[row:row\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m, col:col\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mbetas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mM\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m:\u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbetas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgrad\u001b[49m\n\u001b[1;32m     14\u001b[0m v[row:row\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m, col:col\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m betas[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39mv[row:row\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m, col:col\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mbetas[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m*\u001b[39mgrad\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     15\u001b[0m M_hat \u001b[38;5;241m=\u001b[39m M[row:row\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m, col:col\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mbetas[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mT[row:row\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m, col:col\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "def get_target(labels):\n",
    "    a = random.randint(0, 9)\n",
    "    while a == labels[0]:\n",
    "        a = random.randint(0, 9)\n",
    "    return torch.tensor([a])\n",
    "\n",
    "\n",
    "\n",
    "total = 0\n",
    "success = 0\n",
    "num_image = 10 # number of images to be attacked\n",
    "\n",
    "true_images, adv_images = [], []\n",
    "\n",
    "for i, (images, labels) in enumerate(testloader):\n",
    "    target_label = get_target(labels)\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    outputs = model(images)\n",
    "    _, predicted = outputs.max(1)\n",
    "    if predicted.item() != labels.item():\n",
    "        continue\n",
    "\n",
    "    total += 1\n",
    "\n",
    "    #adv_image = zoo_attack(network=model, image=images, target=target_label)\n",
    "    adv_image = zoo_attack_adam(network=model, image=images, label=labels)\n",
    "    adv_image = adv_image.to(device)\n",
    "    adv_output = model(adv_image)\n",
    "    _, adv_pred = adv_output.max(1)\n",
    "    if adv_pred.item() != labels.item():\n",
    "        success += 1\n",
    "    true_images.append(images.permute([2,3,0,1])[:,:,:,0].detach().cpu().numpy())\n",
    "    adv_images.append(adv_image.permute([2,3,0,1])[:,:,:,0].detach().cpu().numpy())\n",
    "    utils.save_image(adv_image, './adv_images/'+str(total)+''+str(adv_pred.item())+''+str(labels.item())+'_image.png')\n",
    "    \n",
    "    del images\n",
    "    del adv_image\n",
    "    \n",
    "    # print(F.mse_loss(images, adv_image))\n",
    "    print('\\rsuccess rate : %.4f'%(success/total), end=\"\")\n",
    "    if total >= num_image:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b41749e-5f70-4d17-b543-e334a4b1de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "for i in range(0,18,2):\n",
    "    plt.subplot(3,6,i+1)\n",
    "    plt.imshow(true_images[i], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(3,6,i+2)\n",
    "    plt.imshow(adv_images[i], cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1829f57c-503f-4a7d-9446-b9b5d25d0562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
